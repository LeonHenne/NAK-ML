{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaMoDRR00NWz"
      },
      "source": [
        "# **Sneezy defeating Google Recaptcha**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4poTTNG4FeO"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htm1TnlN37qS"
      },
      "source": [
        "Thema: Eine interessante und lehrreiche Datenanalyse auf einem von Ihnen wählbaren Datenset\n",
        "\n",
        "Einschränkung: Keines der \"klassischen\" Datensets aus scikit-learn oder Keras.\n",
        "Die Arbeit soll Ihre Kompetenzen im Bereich Maschinelles Lernen demonstrieren.\n",
        "Die Arbeit soll die Bereiche \"Domainverständnis\", \"Datenvorverarbeitung\", \"Analyse\" und \"Visualisierung\" abdecken.\n",
        "Sie können sich an anderen Arbeiten orientieren, müssen das Gelernte dann aber auf Ihren gewählten Analysegegenstand übertragen.\n",
        "Verwendete Quellen müssen im Notebook angegeben werden.\n",
        "Format: Ein vollständiges und in sich abgeschlossenes Jupyter Notebook\n",
        "\n",
        "Das vollständig ausgeführte Jupyter Notebook ist zusätzlich auch als PDF-Datei einzureichen.\n",
        "Falls die analysierten Daten zu umfangreich sind um sie mitabzugeben, reicht ein Link auf das Datenset.\n",
        "Gruppengröße: 4 Personen (in Sonderfällen 3 Personen)\n",
        "\n",
        "Bearbeitungszeitraum: **16.08 - 08.09.2023**\n",
        "\n",
        "**23.08: Einreichung einer Projektskizze** (ca. eine DIN A4-Seite): untersuchte Daten, gewählte Fragestellung, geplantes Vorgehen, Aufgabenverteilung in der Gruppe\n",
        "\n",
        "**30.08: Abgabe eines Zwischenstands** (lauffähiges Jupyter Notebook) und eines Zwischenberichts (ca. eine DIN A4-Seite): erreichter Stand, aufgetretene Herausforderungen, begründete Abweichungen von der Projektskizze\n",
        "\n",
        "**08.09: Abgabe der finalen Version (vollständiges Jupyter Notebook + Erklärung)**\n",
        "Erklärung: Unterschriebene Eigenständigkeitserklärung + Aufschlüsselung der Arbeitsaufteilung innerhalb der Gruppe (Hauptverantwortlichkeiten für Bestandteile + individueller Beitrag in Prozent der Gesamtleistung)\n",
        "\n",
        "Arbeitsumfang: 40 - 50 Arbeitsstunden pro Person\n",
        "\n",
        "Bewertungskriterien laut Masterhausarbeitsvorlage:\n",
        "\n",
        "Gliederung der Arbeit / Aufbau und Darstellung der Problemstellung / Systematik / Struktur (\"roter Faden\")\n",
        "Wissenschaftlichkeit / Inhaltliche Vollständigkeit und Richtigkeit / Themenrelevanz / Quellenarbeit / Eigenleistung\n",
        "Klarheit der Darstellung & Stringenz der Argumentationskette / formale Korrektheit / Rechtschreibung / Schreibstil\n",
        "Zielpublikum: Studierende Ihres Studiengangs\n",
        "\n",
        "Fokus: Demonstration Ihrer Kompetenzen + Wissensvermittlung (das konkrete Analyseergebnis ist nachrangig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyVckKxoH0lc"
      },
      "source": [
        "## Projektskizze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr888BT9Hauu"
      },
      "source": [
        "### Google reCAPTCHA V2\n",
        "\n",
        "\n",
        "#### Hintergrund und Kontext\n",
        "\n",
        "[Google reCAPTCHA](https://developers.google.com/recaptcha/docs/display) ist ein Sicherheitswerkzeug, das entwickelt wurde, um zwischen menschlichen Benutzern und Bots zu unterscheiden und so Missbrauch und Cyberangriffe zu verhindern. Die Version, reCAPTCHA V2, stellt Benutzern Herausforderungen wie die Identifizierung von Objekten in Bildern. Das Umgehen dieser Sicherheitsmaßnahme mittels maschinellen Lernens und tiefen neuronalen Netzen ist sowohl technisch als auch wissenschaftlich interessant, da es die Leistungsfähigkeit dieser Modelle testet und gleichzeitig zur Verbesserung der Sicherheit von CAPTCHA-Systemen beitragen kann.\n",
        "\n",
        "#### Fragestellungen und Ziele\n",
        "\n",
        "Zu welcher Genauigkeit können derzeit Modelle mittels Verfahren des maschinellen Lernens optimiert werden, um in der Anwendung Google’s Recaptha V2 Bilder korrekt zu klassifizieren?\n",
        "\n",
        "- Welche Veränderung der Modellgüte kann mit aktuellen Methoden der Vorverarbeitung und der Datenaugmentierung aus Forschung und Praxis erzielt werden?\n",
        "\n",
        "- Was sind die neuesten Entwicklungen (State-of-the-Art) in der Bildverarbeitung mit maschinellem Lernen, insbesondere bei der Verwendung von tiefen neuronalen Netzen wie Inceptionv3?  \n",
        "\n",
        "- Welche in der Forschung bestehenden Metriken zur Klassifikation eignen sich zur Lösung des oben beschriebenen Anwendungsfalls?\n",
        "\n",
        "#### Geplantes Vorgehen und Aufgabenverteilung\n",
        "\n",
        "- Explorative Datenanalyse (EDA): Untersuchung der Daten durch Visualisierungen und deskriptive Statistiken, um erste Einblicke zu gewinnen. (Hauptverantwortlich: Rares, Niklas)\n",
        "\n",
        "- Datenvorbereitung: Erstellen eines geeigneten Datensatzes durch Resizing, Resampling und extrahieren von Labels aus der Ordnerstruktur (Paarprogrammierung)\n",
        "\n",
        "- Modellierung: Auswahl und Anwendung geeigneter statistischer Modelle oder Algorithmen zur Beantwortung der Fragestellung. (Hauptverantwortlich: Leon)\n",
        "\n",
        "- Ergebnisse und Interpretation: Analyse der Ergebnisse der Modelle, Interpretation der Befunde und Vergleich mit bestehenden Theorien. (Paarprogrammierung)\n",
        "\n",
        "- Berichterstattung: Erstellung eines detaillierten Berichts, der die Ergebnisse zusammenfasst und Empfehlungen basierend auf den Befunden gibt. (Paarprogrammierung)\n",
        "\n",
        "Das Projekt wird im Google Colab1 entwickelt.\n",
        "\n",
        "#### Literatur und Quellen\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/cry2003/google-recaptcha-v2-images\n",
        "\n",
        "Notebook – InceptionV3: https://www.kaggle.com/code/ahmedhossam666/google-recapthca\n",
        "\n",
        "ResNet Paper: https://arxiv.org/abs/1512.03385\n",
        "\n",
        "Google RecapthaV2: https://developers.google.com/recaptcha/docs/display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimentparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datenparameter (2 Ausprägungen)\n",
        "- Methoden zur Qualitätsverbesserung von Bilddatensätzen\n",
        "  - Data Augmentation (Ohne, Mit (0.05, 0.05, True))\n",
        "    - Zoom (Begründen dass dadurch relevante inhalte fehlen können (Ampel))\n",
        "    - Kontrast\n",
        "      - 0.05 / 0.025 (Mountain)\n",
        "    - Helligkeit\n",
        "      - 0.05 / 0.025 (Mountain)\n",
        "    - horizontale Spiegelung\n",
        "      - horizontale Spiegelung\n",
        "\n",
        "  - Datenbalancierung (Ohne, Mit)\n",
        "    - Erzeugung eines balancierten Datensatzes mittels Datenaugmentierung\n",
        "\n",
        "- Art der Bereitstellung\n",
        "  - Stapelgröße (batch_size)\n",
        "    - 64\n",
        "\n",
        "Trainingsparameter (3 Ausprägungen)\n",
        "- Modellhyperparameter\n",
        "  - Optimierungsalgorithmus\n",
        "    - adam\n",
        "  - Verlustmetrik\n",
        "    - kategorische Kreuzentropie (categorical_crossentropy)\n",
        "  - Lernrate\n",
        "    - 0.001\n",
        "\n",
        "- Verwendete neuronale Netzarchitekturen\n",
        "  - ResNet50V2\n",
        "  - InceptionV3\n",
        "  - LeNet 5 Architecture (Eigenentwicklung anhand der Literatur)\n",
        "\n",
        "- Untersuchte Metriken (werden immer alle betrachtet)\n",
        "  - Accuracy\n",
        "  - f1-score\n",
        "  - Precision\n",
        "  - Recall\n",
        "\n",
        "  Insgesamt ergeben sich ((2 * 2) -1) * 3 = 9 zu untersuchende Kombinationen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In der Entwicklung dieser Arbeit wird die Programmiersprache Python in ihrer Version 3.10.14 verwendet.\n",
        "\n",
        "Alle benötigten zusätzlichen Bibliotheken können den je nach Betriebssystem unterschiedlichen Textdateien im Ordner ``setup`` entnommen werden.\n",
        "Die Installation der Bibliotheken kann z.B in einer virutellen Umgebung durch *conda* oder *venv* mittels dem Befehl ``pip install setup/windows/requirements.txt``, bzw. ``pip install setup/mac/requirements.txt`` durchgeführt werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7g25uqWgbY"
      },
      "source": [
        "## 0. Unterstützende Funktionen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dieses Kapitel dient der definition von Hilfsfunktionen.\n",
        "\n",
        "Dabei wird ein Objekt des Logging Modules genutzt, um zusätzliche Informationen auszugeben und unterschiedliche Detailgrade (info, debug) zu ermöglichen. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "K0_cK93eWj43"
      },
      "outputs": [],
      "source": [
        "\"\"\" This module defines the logging component.\"\"\"\n",
        "import logging\n",
        "\n",
        "\n",
        "def create_logger(log_level: str, logger_name: str = \"custom_logger\"):\n",
        "    \"\"\"Create a logging based on logger.\n",
        "\n",
        "    Args:\n",
        "        log_level (str): Kind of logging\n",
        "        logger_name (str, optional): Name of logger\n",
        "\n",
        "    Returns:\n",
        "        logger: returns logger\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(logger_name)\n",
        "    logger.setLevel(logging.DEBUG) \n",
        "   \n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "\n",
        "    console_handler = logging.StreamHandler()\n",
        "    if log_level == \"DEBUG\":\n",
        "        console_handler.setLevel(logging.DEBUG)\n",
        "    elif log_level == \"INFO\":\n",
        "        console_handler.setLevel(logging.INFO)\n",
        "    elif log_level == \"WARNING\":\n",
        "        console_handler.setLevel(logging.WARNING)\n",
        "    elif log_level == \"ERROR\":\n",
        "        console_handler.setLevel(logging.ERROR)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid log level provided\")\n",
        "\n",
        "  \n",
        "    formatter = logging.Formatter(\n",
        "        \"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "        datefmt=\"%Y-%m-%d %H-%M-%S\",\n",
        "    )\n",
        "    console_handler.setFormatter(formatter)\n",
        "\n",
        " \n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGt1M53No4P5"
      },
      "outputs": [],
      "source": [
        "logger = create_logger(\n",
        "    log_level=\"INFO\",\n",
        "    logger_name=__name__,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGkiRdzlq5sF"
      },
      "source": [
        "## 1. Laden der Daten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgvQKAj5vHOw"
      },
      "source": [
        "Google Drive in Google Colab-Notebook einbinden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9th75We80Hqh",
        "outputId": "d48b4599-848f-46d0-9a31-9679c0bfa516"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr6oB38u5H8R"
      },
      "source": [
        "Erstelle einen Ordner auf deinem lokalen Laufwerk und navigiere hinein\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWkocNqb1e_O",
        "outputId": "fab01458-84b8-4f25-9b12-4e3b03f5877f"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# file_path = '/content/drive/MyDrive/MADS2400'\n",
        "# if os.path.isdir(file_path):\n",
        "#   %cd /content/drive/MyDrive/MADS2400\n",
        "# else:\n",
        "#   %mkdir /content/drive/MyDrive/MADS2400\n",
        "#   %cd /content/drive/MyDrive/MADS2400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4XxXy9bFfyg"
      },
      "source": [
        "Durch die Nutzung eines Tokens wird der Datensatz lokal, oder auf dem Google Drive kopiert.\n",
        "\n",
        "Dabei ist es bewährte Praxis, Tokens und andere sensitive Informationen in einer environment datei (.env) zu speichern.\n",
        "\n",
        "Folgend wird demnach eine *.env* Datei erwartet, in welcher der Token als Variable *git_fine_grained_token* gespeichert ist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngNy8K222mZA",
        "outputId": "4277b0c3-9a6c-4fe4-f0c7-c748a36095dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "load_path = 'Google-Recaptcha-V2-Images'\n",
        "# TODO Do not forget about the token!!!\n",
        "if not os.path.isdir('/content/drive/MyDrive/MADS2400/Google-Recaptcha-V2-Images') or not os.path.isdir(load_path):\n",
        "  git_fine_grained_token = os.environ[\"git_fine_grained_token\"]\n",
        "  username = 'RaresMihai11'\n",
        "  repository = 'Google-Recaptcha-V2-Images'\n",
        "  !git clone https://{git_fine_grained_token}@github.com/{username}/{repository}\n",
        "else:\n",
        "  logger.info(\"Datenrepository wurde bereits geladen\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2CFg5j0MgXW"
      },
      "source": [
        "## 2. Explorative Datenanalyse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8bzEj_S4Qe"
      },
      "source": [
        "### 2.1 Verzeichnisstruktur und Dateianzahl\n",
        "Nach dem erfolgreichen Transfer der Daten auf das lokale Laufwerk ist der erste Schritt die Untersuchung der Verzeichnisstruktur und der Dateianzahl. Dies ermöglicht einen Überblick über die Organisation und die Verteilung der Daten innerhalb der verschiedenen Ordner.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFPWAjXmhmmL",
        "outputId": "e81715fd-3631-4879-fbea-203b3bf8f484"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "main_dir = './Google-Recaptcha-V2-Images/'\n",
        "folders = [\"Bicycle\", \"Bridge\", \"Bus\",\"Car\", \"Chimney\", \"Crosswalk\", \n",
        "           \"Hydrant\", \"Motorcycle\", \"Mountain\", \"Other\", \"Palm\", \"Stair\", \"TLight\"]\n",
        "\n",
        "folder_image_data = {}\n",
        "\n",
        "for folder in folders:\n",
        "\n",
        "    folder_path = os.path.join(main_dir, folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_files = []\n",
        "\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "                with Image.open(file_path) as img:\n",
        "                    img_format = img.format\n",
        "                    image_files.append({\n",
        "                        'image': img.copy(),\n",
        "                        'format': img_format\n",
        "                    })\n",
        "                    \n",
        "        folder_image_data[folder] = {\n",
        "            \"count\": len(image_files),\n",
        "            \"images\": image_files\n",
        "        }\n",
        "\n",
        "        logger.debug(f'{folder}: {len(image_files)} Bilder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HWHQbh1zDRp"
      },
      "source": [
        "### 2.2 Klassenverteilung\n",
        "Die Anzahl der Bilder in jeder Klasse wurde überprüft und im Balkendiagramm dargestellt. Es ist zu erkennen, dass der Datensatz nicht ausgeglichen ist. Das kann zu einem Bias im Modell führen und die Modellleistung beeinträchtigen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "wqIbhqNFzFpx",
        "outputId": "5004294a-6a98-42ef-cf43-2520d78ad29e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "folders = folder_image_data.keys()\n",
        "values = [folder['count'] for folder in folder_image_data.values()]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(folders, values, color='blue', edgecolor='black')\n",
        "plt.xlabel('Klassen')\n",
        "plt.ylabel('Anzahl der Bilder')\n",
        "plt.title('Verteilung der Bilder pro Klasse')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1OTEN_0Kcd"
      },
      "source": [
        "### 2.3 Dateiendungen prüfen und Konvertierung in JPG\n",
        "Die Verteilung der Bildformate (z.B. PNG, JPG) wurde untersucht, um sicherzustellen, dass alle Formate berücksichtigt werden. Eine dritte Kategorie für alle anderen Dateiendungen wurde erstellt.\n",
        "\n",
        "Die Bildformate JPG und PNG weisen zum Beispiel unterschiedliche Eigenschaften auf, insbesondere in Bezug auf die Kanäle (Channels), die sie verwenden. Daher wird auf ein Format konvertiert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "xiPSWcjBItp5",
        "outputId": "683cfa51-a59c-4752-ff63-c0eb88ef7e31"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def check_file_types():\n",
        "  file_format_counts = {'PNG': 0, 'JPG': 0, 'JPEG': 0, 'other': 0}\n",
        "\n",
        "  for folder, data in folder_image_data.items():\n",
        "      for img in data['images']:\n",
        "          img_format = img['format'] if img['format'] else 'other'\n",
        "\n",
        "          if img_format in file_format_counts:\n",
        "              file_format_counts[img_format] += 1\n",
        "          else:\n",
        "              file_format_counts['other'] += 1\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(file_format_counts.keys(), file_format_counts.values(), color='blue', edgecolor='black')\n",
        "  plt.xlabel('Image Format')\n",
        "  plt.ylabel('Number of Images')\n",
        "  plt.title('Distribution of Image Formats')\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.show()\n",
        "\n",
        "check_file_types()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "MlE6ZXc1DUUi",
        "outputId": "fcd913a9-ba58-4ce6-c92e-bdd732b022cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "count = 0\n",
        "for folder, data in folder_image_data.items():\n",
        "    converted_images = []\n",
        "\n",
        "    for img in data['images']:\n",
        "        if img['format'] == 'PNG':\n",
        "            count += 1\n",
        "            img_rgb = img['image'].convert('RGB')\n",
        "            converted_images.append({'image': img_rgb, 'format': 'JPEG'})\n",
        "        else:\n",
        "            converted_images.append(img)\n",
        "\n",
        "    folder_image_data[folder]['images'] = converted_images\n",
        "\n",
        "logger.info(f\"Insgesamt {count} Bilder konvertiert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "0elG9YDbVP7y",
        "outputId": "94d8266d-b6dc-43df-971c-ba95d89df6fa"
      },
      "outputs": [],
      "source": [
        "check_file_types()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkFvsNzK2L92"
      },
      "source": [
        "### 2.4 Bildgrößen und Auflösungen\n",
        "Die Verteilung der Bildbreiten und -höhen wurde visualisiert, um ein besseres Verständnis der Größenverteilung innerhalb des Datensatzes zu erlangen. Diese Information ist entscheidend für die Entscheidung über die Bildskalierung und -normalisierung in späteren Schritten der Datenvorverarbeitung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8chIcxSomzkq",
        "outputId": "4037bd6e-6139-4c15-c627-edd3392c7058"
      },
      "outputs": [],
      "source": [
        "image_sizes = set()\n",
        "\n",
        "for folder, data in folder_image_data.items():\n",
        "    for img_data in data['images']:\n",
        "        img = img_data['image']\n",
        "        size = img.size\n",
        "        image_sizes.add(size)\n",
        "\n",
        "\n",
        "unique_sizes = list(image_sizes)\n",
        "\n",
        "logger.info(f\"Menge an unterschiedlichen Bildgrößen: {unique_sizes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7uBt2TK8_Ls"
      },
      "source": [
        "### 2.5 Plotte ein Bild aus jedem Ordner\n",
        "Ein visueller Eindruck des Datensatzes wurde durch das Anzeigen von Beispielbildern aus jedem Ordner gewonnen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d8VyG9bDVi2x",
        "outputId": "956b7682-6e01-4ec8-bc95-3a4331a662df"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "count = 0\n",
        "axes = axes.flatten()\n",
        "\n",
        "for folder, data in folder_image_data.items():\n",
        "    if count >= 16:\n",
        "        break\n",
        "\n",
        "    images = data['images']\n",
        "    if images:\n",
        "        img_dict = images[0]\n",
        "        img = img_dict['image']\n",
        "        axes[count].imshow(img)\n",
        "        axes[count].set_title(folder)\n",
        "        axes[count].axis('off')\n",
        "        count += 1\n",
        "\n",
        "for i in range(count, 16):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKD5GyA05FiS"
      },
      "source": [
        "### 2.6 Zusammenfassung und Schlussfolgerungen der EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUdGaFGR5ILf"
      },
      "source": [
        "**Erkenntnisse**\n",
        "\n",
        "Verteilung der Bildanzahlen: Die meisten Bilder sind in der Kategorie „Car“ vorhanden, was möglicherweise darauf hinweist, dass diese Kategorie am häufigsten vorkommt oder für die reCAPTCHA-Herausforderungen am wichtigsten ist.\n",
        "\n",
        "Kategorien mit wenig Bildern: Kategorien wie „Mountain“ haben nur sehr wenige Bilder, was zu einer Ungleichheit in der Datenmenge führen kann.\n",
        "\n",
        "Mögliche Anomalien: Kategorien wie „Mountain“ könnten als mögliche Anomalien betrachtet werden, die weitere Aufmerksamkeit erfordern.\n",
        "\n",
        "**Empfehlungen**\n",
        "\n",
        "Datenbalance: Eine Datenbalancierung könnte notwendig sein, um sicherzustellen, dass das Modell gleichmäßig über alle Kategorien trainiert wird.\n",
        "\n",
        "Weitere Datensammlung: Für Kategorien mit wenigen Bildern könnten zusätzliche Daten gesammelt werden, um die Modellleistung zu verbessern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZWKkxZ24Y2H"
      },
      "source": [
        "## 3. Erstellen eines Trainingsdatensatzes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWq75h_X4eWK"
      },
      "source": [
        "Das Ziel dieses Abschnitts ist die Erzeugung eines Datensatzes, welcher für die maschinelle Verarbeitung geeignet ist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Konfiguration der Experimentparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zur besseren Lesbarkeit und Wartbarkeit wurden Datenvorbereitung-, Training- und Modellauswahl-Parameter generalisiert und an einer zentralen Stelle definiert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "HP_AUGMENT_DATA = hp.HParam('augment_data', hp.Discrete([True, False])) \n",
        "HP_DATA_BALANCE = hp.HParam('data_balance', hp.Discrete([True, False])) \n",
        "\n",
        "HP_DATA_ZOOM = hp.HParam('data_zoom', hp.RealInterval(-1.0, 1.0)) \n",
        "HP_DATA_CONTRAST = hp.HParam('data_contrast', hp.RealInterval(-1.0, 1.0)) \n",
        "HP_DATA_BRIGHTNESS_LOW = hp.HParam('data_brightness_low', hp.RealInterval(-1.0, 1.0)) \n",
        "HP_DATA_BRIGHTNESS_UP = hp.HParam('data_brightness_up', hp.RealInterval(-1.0, 1.0)) \n",
        "HP_DATA_FLIP = hp.HParam('data_flip', hp.Discrete([True, False])) \n",
        "\n",
        "HP_TRAINING_LOSS = hp.HParam('loss', hp.Discrete(['categorical_crossentropy']))\n",
        "HP_TRAINING_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "HP_TRAINING_EPOCHS = hp.HParam('epochs', hp.IntInterval(1, 10))\n",
        "\n",
        "HP_MODEL_SELECTION = hp.HParam('model', hp.Discrete(['resnet50v2', 'inceptionv3', 'leNet5']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Folgend sind 3 Code-Blöcke mit unterschiedlichen Parametereinstellungen definiert. \n",
        "Für die Auswertung der Forschungsfragen wurden diese Einstellungen der Parameter untersucht.\n",
        "\n",
        "Beim Durchführen des Notebooks gilt es eine der Parametereinstellung als *hparams* zu speichern. \n",
        "Alle später definierten Modelle werden dann auf der jeweiligen Weisen trainiert:\n",
        "\n",
        "\t•\tKeine Augmentation und keine Datenbalancierung\n",
        "\t•\tAugmentation und keine Datenbalancierung\n",
        "\t•\tAugmentation und Datenbalancierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_augment_unbalanced = {\n",
        "    HP_AUGMENT_DATA: False,\n",
        "    HP_DATA_BALANCE: False,\n",
        "\n",
        "    HP_DATA_ZOOM: 0.05,\n",
        "    HP_DATA_CONTRAST: 0.05,\n",
        "    HP_DATA_BRIGHTNESS_LOW: -0.05,\n",
        "    HP_DATA_BRIGHTNESS_UP: 0.05, \n",
        "    HP_DATA_FLIP: True,\n",
        "\n",
        "    HP_TRAINING_LOSS: 'categorical_crossentropy',\n",
        "    HP_TRAINING_OPTIMIZER: 'adam',\n",
        "    HP_TRAINING_EPOCHS: 10,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augment_unbalanced = {\n",
        "    HP_AUGMENT_DATA: True,\n",
        "    HP_DATA_BALANCE: False,\n",
        "\n",
        "    HP_DATA_ZOOM: 0.05,\n",
        "    HP_DATA_CONTRAST: 0.05,\n",
        "    HP_DATA_BRIGHTNESS_LOW: -0.05,\n",
        "    HP_DATA_BRIGHTNESS_UP: 0.05, \n",
        "    HP_DATA_FLIP: True,\n",
        "\n",
        "    HP_TRAINING_LOSS: 'categorical_crossentropy',\n",
        "    HP_TRAINING_OPTIMIZER: 'adam',\n",
        "    HP_TRAINING_EPOCHS: 10,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augment_balanced = {\n",
        "    HP_AUGMENT_DATA: True,\n",
        "    HP_DATA_BALANCE: True,\n",
        "    HP_DATA_ZOOM: 0.05,\n",
        "    HP_DATA_CONTRAST: 0.05,\n",
        "    HP_DATA_BRIGHTNESS_LOW: -0.05,\n",
        "    HP_DATA_BRIGHTNESS_UP: 0.05, \n",
        "    HP_DATA_FLIP: True,\n",
        "\n",
        "    HP_TRAINING_LOSS: 'categorical_crossentropy',\n",
        "    HP_TRAINING_OPTIMIZER: 'adam',\n",
        "    HP_TRAINING_EPOCHS: 10,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams = no_augment_unbalanced #augment_unbalanced #augment_balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex-RuJmAdWhk"
      },
      "source": [
        "### 3.2 Daten augumentieren\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Frage, wie stark man Daten durch Augmentierung erhöhen sollte, ist ein wichtiger Forschungsbereich im maschinellen Lernen. Die Praxis, Datenklassen auf ähnliche Größenordnungen zu bringen, basiert auf dem Bedürfnis, das Modell vor Überanpassung an eine bestimmte Klasse zu schützen und sicherzustellen, dass alle Klassen ausreichend repräsentiert sind. Es gibt jedoch verschiedene Studien und Richtlinien, die aufzeigen, dass eine signifikante Augmentierung sinnvoll sein kann:\n",
        "\n",
        "Studien zur Verbesserung der Klassifikationsleistung durch Datenaugmentierung:\n",
        "\n",
        "Wong et al. (2016) in ihrem Paper \"Understanding Data Augmentation for Classification\" zeigen, dass Augmentierung insbesondere bei kleinen Datensätzen die Generalisierungsfähigkeit eines Modells erheblich verbessern kann. Sie heben hervor, dass selbst drastische Erhöhungen der Datenmenge durch Augmentierung (um das 10- bis 100-fache) bei unterrepräsentierten Klassen zu einer verbesserten Leistung führen können.\n",
        "Perez und Wang (2017) in \"The Effectiveness of Data Augmentation in Image Classification using Deep Learning\" analysieren den Effekt von Datenaugmentierung in verschiedenen Szenarien und finden heraus, dass eine drastische Erhöhung der Anzahl von Trainingsbeispielen durch Augmentierung zu einer signifikant besseren Leistung führen kann, insbesondere wenn die Augmentierungen realistische Varianten der Bilder erzeugen.\n",
        "Balance von Klassen:\n",
        "\n",
        "Buda, Maki, und Mazurowski (2018) in ihrem Paper \"A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks\" untersuchen die Auswirkungen von Klassenungleichgewichten und zeigen, dass ein Ausgleich der Klassenverteilung durch Datenaugmentierung oder andere Techniken entscheidend ist, um die Performance eines Modells zu verbessern. Sie betonen, dass eine zu große Diskrepanz in der Klassenverteilung zu einer schlechteren Modellleistung führt.\n",
        "Praktische Leitlinien:\n",
        "\n",
        "Die Praxis der \"Over-Sampling\"-Technik durch Augmentierung (bei der unterrepräsentierte Klassen stark augmentiert werden) ist eine weit verbreitete Methode, um Klassifikationsmodelle robuster zu machen. Es gibt keine festgelegte Grenze für das \"Wie viel\", aber es ist üblich, die kleineren Klassen auf eine ähnliche Größe wie die größeren zu bringen, um ein ausgewogenes Training zu ermöglichen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2.1 Augmentierung der Daten durch Zoom, Kontrast, Helligkeit und vertikale Spiegelung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3.2.1.1 Definition und Ausführung der Augmentierungspipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "augmentation_pipeline() erstellt eine Augmentierungspipeline basierend auf übergebenen Parametern wie Zoom, Kontrast, Helligkeit und Spiegelung. Die Funktion augment_class_images() nimmt Bilddaten eines bestimmten Ordners und führt Augmentierungen durch, um die Anzahl der Bilder auf eine gewünschte Zielmenge zu erhöhen. Augmentierte Bilder werden der Liste hinzugefügt, bis die gewünschte Bildanzahl erreicht ist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2S8wUPadctR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def augmentation_pipeline(zoom:float = hparams[HP_DATA_ZOOM], \n",
        "                          contrast:float = hparams[HP_DATA_CONTRAST], \n",
        "                          brightness:list = [hparams[HP_DATA_BRIGHTNESS_LOW],hparams[HP_DATA_BRIGHTNESS_UP]],\n",
        "                          flip: bool = hparams[HP_DATA_FLIP]):\n",
        "\n",
        "    if hparams[HP_DATA_FLIP]:\n",
        "        logger.debug(f\"Augmentierungs Parameter: Zoom - +/- {zoom} %, Kontrast - +/- {contrast} %, Helligkeit - -{brightness[0]}% bis +{brightness[0]}%, Spiegelung - {'Trifft zu' if flip == True else 'Trifft nicht zu'}\")\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomZoom(height_factor=zoom, seed=42),\n",
        "            tf.keras.layers.RandomContrast(factor=contrast, seed=42),\n",
        "            tf.keras.layers.RandomBrightness(factor=brightness, seed=42),\n",
        "            tf.keras.layers.RandomFlip(mode='horizontal', seed=42), \n",
        "        ])\n",
        "    else:\n",
        "        logger.debug(f\"Augmentierungs Parameter: Zoom - +/- {zoom} %, Kontrast - +/- {contrast} %, Helligkeit - -{brightness[0]}% bis +{brightness[0]}%, Spiegelung - {'Trifft zu' if flip == True else 'Trifft nicht zu'}\")\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomZoom(height_factor=zoom, seed=42),\n",
        "            tf.keras.layers.RandomContrast(factor=contrast, seed=42),\n",
        "            tf.keras.layers.RandomBrightness(factor=brightness, seed=42),\n",
        "        ])\n",
        "    return data_augmentation\n",
        "\n",
        "def augment_class_images(folder:str, folder_data, target_count):\n",
        "    \n",
        "    current_count = folder_data['count']\n",
        "    images = folder_data['images']\n",
        "    if folder == \"Mountain\":\n",
        "        data_augmentation = augmentation_pipeline(contrast=0.025, brightness=[-0.025,0.025])\n",
        "    else:\n",
        "        data_augmentation = augmentation_pipeline()\n",
        "\n",
        "    while current_count < target_count:\n",
        "        for img_dict in images:\n",
        "            img = img_dict['image']\n",
        "            img_array = np.array(img.convert('RGB'))\n",
        "\n",
        "            img_augmented = data_augmentation(tf.expand_dims(img_array, 0))\n",
        "            img_augmented = tf.squeeze(img_augmented).numpy().astype(\"uint8\")\n",
        "\n",
        "            img_augmented_pil = Image.fromarray(img_augmented)\n",
        "\n",
        "            images.append({'image': img_augmented_pil})\n",
        "            current_count += 1\n",
        "\n",
        "            if current_count >= target_count:\n",
        "                break\n",
        "\n",
        "    folder_data['count'] = current_count\n",
        "    folder_data['images'] = images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if hparams[HP_AUGMENT_DATA] == True:\n",
        "\n",
        "    for folder, data in folder_image_data.items():\n",
        "        logger.info(f\"Augmentiere Bilder der Klasse: {folder}\")\n",
        "        target_count = 2000\n",
        "        if data['count'] < target_count:\n",
        "            augment_class_images(folder, data, target_count)\n",
        "        elif hparams[HP_DATA_BALANCE] == True and data['count'] > target_count:\n",
        "            logger.info(f\"Reduziere Bilder der Klasse: {folder} auf {target_count}.\")\n",
        "            data['images'] = data['images'][:target_count]\n",
        "            data['count'] = target_count  \n",
        "        logger.info(f\"Klasse {folder} enthält nun {data['count']} Bilder.\")\n",
        "    for folder, data in folder_image_data.items():\n",
        "        logger.info(f\"Klasse {folder} enthält nun {data['count']} Bilder.\")\n",
        "else:\n",
        "    logger.debug(f\"HP_AUGMENT_DATA = {hparams[HP_AUGMENT_DATA]}\")\n",
        "    logger.info(f\"Die Daten wurden NICHT augmentiert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 3.2.1.2 Demonstration der Augmentierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "count = 0\n",
        "axes = axes.flatten()\n",
        "\n",
        "data_augmentation = augmentation_pipeline()\n",
        "\n",
        "for folder, data in folder_image_data.items():\n",
        "    if count >= 16:\n",
        "        break\n",
        "    images = data['images']\n",
        "\n",
        "    if images:\n",
        "        img_dict = images[0]\n",
        "        img = img_dict['image']\n",
        "        img_array = np.array(img.convert('RGB'))\n",
        "\n",
        "        axes[count].imshow(img_array)\n",
        "        axes[count].set_title(f\"Original: {folder}\")\n",
        "        axes[count].axis('off')\n",
        "        count += 1\n",
        "\n",
        "        img_augmented = data_augmentation(tf.expand_dims(img_array, 0))\n",
        "        img_augmented = tf.squeeze(img_augmented).numpy().astype(\"uint8\")\n",
        "\n",
        "        axes[count].imshow(img_augmented)\n",
        "        axes[count].set_title(f\"Augmented: {folder}\")\n",
        "        axes[count].axis('off')\n",
        "        count += 1\n",
        "\n",
        "for i in range(count, 16):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besondere Untersuchung der Klasse *Mountain* um die Auswirkungen der Augmentierung visuell zu beurteilen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mountain_images = folder_image_data['Mountain']['images']\n",
        "random.shuffle(mountain_images)\n",
        "selected_images = mountain_images[:81] \n",
        "\n",
        "if hparams[HP_AUGMENT_DATA]:\n",
        "    fig, axes = plt.subplots(9, 9, figsize=(20, 20)) \n",
        "else:\n",
        "    fig, axes = plt.subplots(5, 5, figsize=(10, 10)) \n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, img_dict in enumerate(selected_images):\n",
        "    img = img_dict['image']\n",
        "    img_array = np.array(img.convert('RGB'))\n",
        "    axes[i].imshow(img_array)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "for i in range(len(selected_images), len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2.2 Datensatz in einem temporären Ordner speichern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras kann mit Dictonaries nicht arbeiten, aus diesem Grund werden die Daten in einem temporären Ordner gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import traceback\n",
        "import tempfile\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "try:\n",
        "    for folder, data in folder_image_data.items():\n",
        "        folder_path = os.path.join(temp_dir, folder)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        for i, img_dict in enumerate(data['images']):\n",
        "            img = img_dict['image']\n",
        "            img_rgb = img.convert('RGB')\n",
        "\n",
        "            img_path = os.path.join(folder_path, f'image_{i}.jpg')\n",
        "            img_rgb.save(img_path, format='JPEG')\n",
        "\n",
        "    for root, dirs, files in os.walk(temp_dir):\n",
        "        level = root.replace(temp_dir, '').count(os.sep)\n",
        "        indent = ' ' * 4 * level\n",
        "\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            img_path = os.path.join(root, f)\n",
        "            with Image.open(img_path) as img:\n",
        "                width, height = img.size\n",
        "\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "\n",
        "    count = 0\n",
        "    axes = axes.flatten()\n",
        "    for folder in os.listdir(temp_dir):\n",
        "        if count >= 16:\n",
        "            break\n",
        "        folder_path = os.path.join(temp_dir, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            img_files = os.listdir(folder_path)\n",
        "            if img_files:\n",
        "                img_path = os.path.join(folder_path, img_files[0])\n",
        "                img = Image.open(img_path)\n",
        "                axes[count].imshow(img)\n",
        "                axes[count].set_title(folder)\n",
        "                axes[count].axis('off')\n",
        "                count += 1\n",
        "\n",
        "    for i in range(count, 16):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "  logger.error(\"Fehler in der Erstellung des temporären Ordners\")\n",
        "  traceback.print_exception(*sys.exc_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T9_vSEKuKtM"
      },
      "source": [
        "### 3.3 Datensatz erzeugen mithilfe Keras Bibliotheken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anhand der zur Verfügung stehenden Keras function ```image_dataset_from_directory``` ist es möglich, einfach aus dem bestehenden vorverarbeiteten Bilderverzeichnis zwei in Zahlen kodierte Datensätze zu erzeugen.\n",
        "\n",
        "Mittels dessen Funktionsparameter kann unmittelbar eine Größenanpassung auf 120x120 Pixel und eine Bündelung in Stapel (Batches) je 64 Bildtensoren durchgeführt werden.\n",
        "\n",
        "Die Erkennung des Klasse eines Bildes erfolgt aus der Verortung in ihrem zugehörigen Klassenordner.\n",
        "\n",
        "Eine Klassenbezeichnung im Datensatz wird kategorisch, also als 13-dimensionaler Tensor mit binärer Ausprägung an entsprechender Stelle definiert.\n",
        "\n",
        "Die Sortierung der Klassen im Tensor wird alphanumerisch vorgenommen.\n",
        "\n",
        "|                 |               |\n",
        "|-----------------|---------------|\n",
        "| 1. Bicycle         | 8. Motorcycle    |\n",
        "| 2. Bridge          | 9. Mountain      |\n",
        "| 3. Bus             | 10. Other         |\n",
        "| 4. Car             | 11. Palm          |\n",
        "| 5. Chimney         | 12. Stair         |\n",
        "| 6. Crosswalk       | 13. TLight        |\n",
        "| 7. Hydrant         |               |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWTjI8qZW3ax"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    batch_size = 64\n",
        "\n",
        "    train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        temp_dir,\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=None,\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        image_size=(120, 120),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset='training',\n",
        "        interpolation=\"bilinear\",\n",
        "        follow_links=False,\n",
        "        crop_to_aspect_ratio=False,\n",
        "        pad_to_aspect_ratio=False,\n",
        "        data_format='channels_last',\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        temp_dir,\n",
        "        labels=\"inferred\",\n",
        "        label_mode=\"categorical\",\n",
        "        class_names=None,\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        image_size=(120, 120),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        validation_split=0.2,\n",
        "        subset='validation',\n",
        "        interpolation=\"bilinear\",\n",
        "        follow_links=False,\n",
        "        crop_to_aspect_ratio=False,\n",
        "        pad_to_aspect_ratio=False,\n",
        "        data_format='channels_last',\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Trainingsdatensatz enthält {len(train_dataset)} Stapel je {batch_size} Bilder\")\n",
        "    logger.info(f\"Validierungsdatensatz enthält {len(validation_dataset)} Stapel je {batch_size} Bilder\")\n",
        "\n",
        "    for element in train_dataset:\n",
        "        logger.debug(f\"shape X_train: {element[0].shape}\")\n",
        "        logger.debug(f\"shape Y_train: {element[1].shape}\")\n",
        "        break\n",
        "\n",
        "finally:\n",
        "    logger.info(\"Erfolgreich Trainings- und Validierungsdatensatz erstellt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Folgend werden die erstellten Datensätze visuell überprüft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_class_images(dataset):\n",
        "  category_images = {}\n",
        "  class_names = dataset.class_names\n",
        "  for images, labels in dataset:\n",
        "          for img, label in zip(images, labels):\n",
        "              category = class_names[np.argmax(label)]\n",
        "              if category not in category_images:\n",
        "                  category_images[category] = img.numpy()\n",
        "              if len(category_images) == len(class_names):\n",
        "                  break\n",
        "          if len(category_images) == len(class_names):\n",
        "              break\n",
        "\n",
        "  num_categories = len(category_images)\n",
        "  fig, axes = plt.subplots(1, num_categories, figsize=(15, 5))\n",
        "  for ax, (category, img) in zip(axes, category_images.items()):\n",
        "          ax.imshow(img.astype(\"uint8\"))\n",
        "          ax.set_title(category)\n",
        "          ax.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "show_class_images(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_class_images(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhAkncn0eKvR"
      },
      "source": [
        "## 4. Entwicklung und Optimierung von neuronalen Netzarchitekturen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zur Beantwortung der Forschungsfragen werden zum Einen moderne, vortrainierte Modelle aus dem [Keras Applications-Modul](https://keras.io/api/applications/) untersucht.\n",
        "Diese Modelle wurden auf großen Datensätzen wie ImageNet vortrainiert, was den Vorteil bietet, dass sie für eine Vielzahl von Anwendungen direkt genutzt oder als Basis für das sogenannte Transfer Learning verwendet werden können.\n",
        "Die in dieser Arbeit gestellte Aufgabe zur Klassifizierung von Google Recaptcha Bildern kann dadurch mit minimalem zusätzlichen Training bewältigt werden. \n",
        "Aus der großen Auswahl an vortrainierten Modellen wurden ResNet50V2 und InceptionV3 ausgewählt, da diese unter verhältnismäßig geringer Modellgröße eine gute Leistung beim Training mit dem ImageNet Datensatz aufweisen.\n",
        "\n",
        "Zum Anderen wird die historische Architektur LeNet-5 betrachtet, welche wegweisend für die spätere Entwicklung von Convolutional Neural Networks (CNNs) war. [^1]\n",
        "Ein Vergleich zwischen LeNet-5 und modernen Architekturen wie ResNet oder Inception zeigt die enorme Entwicklung der Komplexität und Leistung von neuronalen Netzen in den letzten Jahrzehnten.\n",
        "\n",
        "[^1]: [LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.](https://ieeexplore.ieee.org/document/726791)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nach der Erstellung der neuronalen Netze beschreibt dieses Notebook die durchgeführten Optimierungsprozesse.\n",
        "Zur Überwachung des Modelltrainings werden verschiedene Callback-Funktionen definiert, um Daten zur Modelleistung sowie das Modell selbst zu speichern.\n",
        "Dazu bietet sich bereits durch die Nutzung der Tensorflow und Keras Bibliotheken die Integration mit Tensorboard an.\n",
        "\n",
        "Die durch Tensorboard exportierten Daten im Ordner *logs* können so später mittels eines interaktiven Dashboard eingesehen werden. \n",
        "Die Metadaten zum Training werden enstprechend der Experimentparameter Art des Datensatzes, Modell und Zeitpunkt sortiert. (i. e. *logs/augment-unbalanced/leNet5/captcha-05.09.2024 23-14-05*)\n",
        "Aus der Notebook Python-Umgebung heraus kann mittels ```tensorboard --logdir ./logs``` ein [lokaler Webserver](http://localhost:6006) gestartet werden, welcher das Dashboard anzeigt.\n",
        "\n",
        "Final kann in [Kapitel 4.4](###44-vorhersage-anhand-optimiertem-modell) nach Angabe des Speicherpfads, eins der optimierten Modelle geladen werden, um die Klassen einer Menge von Bildern vorherzusagen.\n",
        "\n",
        "``Notiz:`` Die gewählte Menge von Bildern soll dabei lediglich dazu dienen die Funktionsweise eines DeepLearning-Modells darzustellen und entspricht keinem zuvor vom Training exkludiertem Testdatensatz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNon5_qxeOq1"
      },
      "source": [
        "### 4.1 Überprüfung der GPU Unterstützung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diese Zelle überprüft die Anbindung der Softwareumgebung an eine GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpMCwpZ5ePTh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as keras\n",
        "import platform\n",
        "\n",
        "print(f\"Python Platform: {platform.platform()}\")\n",
        "print(f\"Python {sys.version}\")\n",
        "print()\n",
        "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version: {keras.__version__}\")\n",
        "print()\n",
        "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
        "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Erstellung verschiedener neuronaler Netzarchitekturen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izSsNMOMeTf2"
      },
      "source": [
        "#### 4.2.1 ResNet50V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zur Anpassung des ResNet50V2 Modells auf die betrachtete Aufgabenstellung, werden darauf aufbauend Schichten dem Modell hinzugefügt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLeuUdjZeQmh"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "base = keras.applications.ResNet50V2(\n",
        "    include_top=False, \n",
        "    weights='imagenet',\n",
        "    input_shape=(120, 120, 3),\n",
        "    name='resnet50v2')\n",
        "\n",
        "resnet50v2 = Sequential()\n",
        "resnet50v2.add(base)\n",
        "resnet50v2.add(Dropout(0.2))\n",
        "resnet50v2.add(Flatten())\n",
        "resnet50v2.add(Dense(13, activation='softmax'))\n",
        "\n",
        "resnet50v2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2.2 InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zur Anpassung des InceptionV3 Modells auf die betrachtete Aufgabenstellung, werden darauf aufbauend Schichten dem Modell hinzugefügt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "\n",
        "base = keras.applications.InceptionV3(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(120, 120, 3),\n",
        "    name=\"inception_v3\",\n",
        ")\n",
        "\n",
        "inceptionv3 = Sequential()\n",
        "inceptionv3.add(base)\n",
        "inceptionv3.add(Dropout(0.2))\n",
        "inceptionv3.add(Flatten())\n",
        "inceptionv3.add(Dense(13, activation='softmax'))\n",
        "\n",
        "inceptionv3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2.3 LeNet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, AveragePooling2D \n",
        "leNet5 = Sequential()\n",
        "leNet5.add(Conv2D(filters=6, strides=1, kernel_size=(5, 5), activation='relu', input_shape=(120, 120, 3)))\n",
        "leNet5.add(AveragePooling2D(strides=2, pool_size=(2, 2)))\n",
        "leNet5.add(Conv2D(filters=16, strides=1, kernel_size=(5, 5), activation='relu',))\n",
        "leNet5.add(AveragePooling2D(strides=2, pool_size=(2, 2)))\n",
        "leNet5.add(Flatten())\n",
        "leNet5.add(Dense(13, activation='softmax'))\n",
        "leNet5.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Trainingsprozess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.1 Überwachung der Trainingsprozesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "if hparams[HP_AUGMENT_DATA]:\n",
        "    dataset_type = 'augment'\n",
        "else:\n",
        "    dataset_type = 'no_augment'\n",
        "\n",
        "if hparams[HP_DATA_BALANCE]:\n",
        "    dataset_type = dataset_type + '-balanced'\n",
        "else:\n",
        "    dataset_type = dataset_type + '-unbalanced'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ProgbarLogger, CSVLogger\n",
        "\n",
        "def create_callbacks(log_dir: str, model_dir: str) -> list:\n",
        "    tensorboard_callback = TensorBoard(\n",
        "        log_dir=log_dir,\n",
        "        histogram_freq=1,\n",
        "        update_freq='epoch',\n",
        "        write_graph=True)\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=model_dir,\n",
        "        save_weights_only=False,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose =1)\n",
        "\n",
        "    progressbar_callback = ProgbarLogger()\n",
        "\n",
        "    csv_callback = CSVLogger(log_dir + 'logs.csv')\n",
        "\n",
        "    callbacks= [tensorboard_callback,\n",
        "                checkpoint_callback,\n",
        "                progressbar_callback,\n",
        "                csv_callback]\n",
        "\n",
        "    return callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.2 Optimierung von ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams[HP_MODEL_SELECTION] = 'resnet50v2'\n",
        "logger.info(f\"Ausgewähltes Modell: {hparams[HP_MODEL_SELECTION]}\")\n",
        "\n",
        "log_dir = f'./logs/{dataset_type}/{hparams[HP_MODEL_SELECTION]}/captcha-{time.strftime(\"%d.%m.%Y %H-%M-%S\", time.localtime())}/'\n",
        "model_dir = f'{log_dir}/models/' + 'model-epoch.{epoch:02d}-val_loss.{val_loss:.2f}-val_acc.{val_accuracy:.2f}.keras'\n",
        "logger.info(f\"Speichere LOGS in: '{log_dir}'\")\n",
        "logger.info(f\"Speichere optimierte Modelle in: '{model_dir}'\")\n",
        "\n",
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "    hp.hparams(hparams)\n",
        "\n",
        "callbacks = create_callbacks(log_dir, model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w63oke0veYV6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import metrics\n",
        "\n",
        "resnet50v2.compile(optimizer=hparams[HP_TRAINING_OPTIMIZER],\n",
        "              loss=hparams[HP_TRAINING_LOSS],\n",
        "              metrics=[\n",
        "                  metrics.CategoricalAccuracy(name = 'accuracy'),\n",
        "                  metrics.Precision(name = 'precision'),\n",
        "                  metrics.Recall(name = 'recall'),\n",
        "                  metrics.AUC(name = 'auc')\n",
        "              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet50v2.fit(train_dataset, epochs=hparams[HP_TRAINING_EPOCHS], validation_data=validation_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.3 Optimierung von InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams[HP_MODEL_SELECTION] = 'inceptionv3'\n",
        "logger.info(f\"Ausgewähltes Modell: {hparams[HP_MODEL_SELECTION]}\")\n",
        "\n",
        "log_dir = f'./logs/{dataset_type}/{hparams[HP_MODEL_SELECTION]}/captcha-{time.strftime(\"%d.%m.%Y %H-%M-%S\", time.localtime())}/'\n",
        "model_dir = f'{log_dir}/models/' + 'model-epoch.{epoch:02d}-val_loss.{val_loss:.2f}-val_acc.{val_accuracy:.2f}.keras'\n",
        "logger.info(f\"Speichere LOGS in: '{log_dir}'\")\n",
        "logger.info(f\"Speichere optimierte Modelle in: '{model_dir}'\")\n",
        "\n",
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "    hp.hparams(hparams)\n",
        "\n",
        "callbacks = create_callbacks(log_dir, model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import metrics\n",
        "\n",
        "inceptionv3.compile(optimizer=hparams[HP_TRAINING_OPTIMIZER],\n",
        "              loss=hparams[HP_TRAINING_LOSS],\n",
        "              metrics=[\n",
        "                  metrics.CategoricalAccuracy(name = 'accuracy'),\n",
        "                  metrics.Precision(name = 'precision'),\n",
        "                  metrics.Recall(name = 'recall'),\n",
        "                  metrics.AUC(name = 'auc')\n",
        "              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inceptionv3.fit(train_dataset, epochs=hparams[HP_TRAINING_EPOCHS], validation_data=validation_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3.4 Optimierung von LeNet5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparams[HP_MODEL_SELECTION] = 'leNet5'\n",
        "logger.info(f\"Ausgewähltes Modell: {hparams[HP_MODEL_SELECTION]}\")\n",
        "\n",
        "log_dir = f'./logs/{dataset_type}/{hparams[HP_MODEL_SELECTION]}/captcha-{time.strftime(\"%d.%m.%Y %H-%M-%S\", time.localtime())}/'\n",
        "model_dir = f'{log_dir}/models/' + 'model-epoch.{epoch:02d}-val_loss.{val_loss:.2f}-val_acc.{val_accuracy:.2f}.keras'\n",
        "logger.info(f\"Speichere LOGS in: '{log_dir}'\")\n",
        "logger.info(f\"Speichere optimierte Modelle in: '{model_dir}'\")\n",
        "\n",
        "with tf.summary.create_file_writer(log_dir).as_default():\n",
        "    hp.hparams(hparams)\n",
        "\n",
        "callbacks = create_callbacks(log_dir, model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import metrics\n",
        "\n",
        "leNet5.compile(optimizer=hparams[HP_TRAINING_OPTIMIZER],\n",
        "              loss=hparams[HP_TRAINING_LOSS],\n",
        "              metrics=[\n",
        "                  metrics.CategoricalAccuracy(name = 'accuracy'),\n",
        "                  metrics.Precision(name = 'precision'),\n",
        "                  metrics.Recall(name = 'recall'),\n",
        "                  metrics.AUC(name = 'auc')\n",
        "              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "leNet5.fit(train_dataset, epochs=hparams[HP_TRAINING_EPOCHS], validation_data=validation_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Vorhersage anhand optimiertem Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "loaded_model = keras.models.load_model(\"logs/augment/inceptionv3/captcha-04.09.2024 22:31:53/models/model-08-0.46-0.85.keras\")\n",
        "\n",
        "\n",
        "folders = [\"Bicycle\", \"Bridge\", \"Bus\", \"Car\", \"Chimney\", \"Crosswalk\", \n",
        "           \"Hydrant\", \"Motorcycle\", \"Mountain\", \"Other\", \"Palm\", \"Stair\", \"TLight\"]\n",
        "\n",
        "\n",
        "image_paths = [\n",
        "    'Google-Recaptcha-V2-Images/Hydrant/0a05f251-260f-4ada-9005-5326e50e1848.jpg',  \n",
        "    'Google-Recaptcha-V2-Images/Bus/0ae6ac38-005c-4519-9e4c-8d72cc7f4d45.jpg',      \n",
        "    'Google-Recaptcha-V2-Images/Bicycle/0b433101-7a68-4b29-b875-ac45c9680489.jpg',  \n",
        "    'Google-Recaptcha-V2-Images/Car/0f8723fa-5ec7-409d-aac9-5e845cdba592.jpg',       \n",
        "    'Google-Recaptcha-V2-Images/Bridge/0a91630a-db06-4fb4-bba1-17ae331db395.jpg',   \n",
        "    'Google-Recaptcha-V2-Images/Chimney/Chimney$95df5fdc7a8d1f84ba73fbc13820b215.png', \n",
        "    'Google-Recaptcha-V2-Images/Crosswalk/7c1cff3c-ed14-4434-a8bb-83f3c80150b8.jpg',\n",
        "    'Google-Recaptcha-V2-Images/TLight/00b93f32-ef8b-4bf1-b80a-0c015e8d49cd.jpg',   \n",
        "    'Google-Recaptcha-V2-Images/Palm/1cc7ea4e-3204-4374-8335-c9f9d2f22dd3.jpg',\n",
        "    'Google-Recaptcha-V2-Images/Stair/Other$7a093d9078b7770b79b371ecbaf1e238.png',\n",
        "    'Google-Recaptcha-V2-Images/Motorcycle/Motorcycle$2e17b45892f5061a2dca5d109f52a304.png',\n",
        "    'Google-Recaptcha-V2-Images/Mountain/Mountain$a971ebef63e9af4221cad93dc9260199.png',\n",
        "    'Google-Recaptcha-V2-Images/Other/Other$0a3119044a5e6776dba11c9b06338c00.png',\n",
        "    'Google-Recaptcha-V2-Images/Mountain/Mountain$399053106902e615176bb63ce1f8b9b3.png',\n",
        "    'Google-Recaptcha-V2-Images/Car/3f3210b9-8116-4025-9619-1b97a1c2b008.jpg',\n",
        "    'Google-Recaptcha-V2-Images/Bridge/4b7b4b28-42e7-4219-9050-0c3233ab6111.jpg'      \n",
        "]\n",
        "\n",
        "\n",
        "real_labels = ['Hydrant', 'Bus', 'Bicycle', 'Car', 'Bridge', 'Chimney', 'Crosswalk', 'TLight', 'Palm', 'Stair', 'Motorcycle', 'Mountain','Other','Mountain','Car', 'Bridge' ]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
        "axs = axs.ravel()\n",
        "\n",
        "for i, img_path in enumerate(image_paths):\n",
        "    img = image.load_img(img_path, target_size=(120, 120))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    prediction = loaded_model.predict(img_array)\n",
        "    predicted_label_index = np.argmax(prediction, axis=1)[0]\n",
        "    predicted_label = folders[predicted_label_index]\n",
        "    \n",
        "    confidence = prediction[0][predicted_label_index] * 100\n",
        "    \n",
        "    axs[i].imshow(image.load_img(img_path))\n",
        "    axs[i].axis('off')  \n",
        "    axs[i].set_title(f'Real: {real_labels[i]}\\nPredicted: {predicted_label}\\nAccuracy: {confidence:.2f}%')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation und Bewertung der Optimierungsprozesse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 RQ 1: Welche Veränderung der Modellgüte kann mit aktuellen Methoden der Vorverarbeitung und der Datenaugmentierung aus Forschung und Praxis erzielt werden?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mit Datenvorverarbeitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ohne Datenvorverarbeitung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Alternativ Nur Zoom, Nur Kontrast, Nur Helligkeit, Nur Spiegelung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 RQ 2: Was sind die neuesten Entwicklungen (State-of-the-Art) in der Bildverarbeitung mit maschinellem Lernen, insbesondere bei der Verwendung von tiefen neuronalen Netzen wie Inceptionv3?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.2.1 ResNet50V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.2.2 InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.2.3 LeNet5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RQ 3: Welche in der Forschung bestehenden Metriken zur Klassifikation eignen sich zur Lösung des oben beschriebenen Anwendungsfalls?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aufteilung der Gruppenleistung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = 'res/aufteilung.csv'\n",
        "df = pd.read_csv(file_path, delimiter =';')\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c4poTTNG4FeO",
        "FyVckKxoH0lc",
        "lu7g25uqWgbY",
        "hGkiRdzlq5sF",
        "X2CFg5j0MgXW",
        "JZ8bzEj_S4Qe",
        "1HWHQbh1zDRp",
        "7d1OTEN_0Kcd",
        "7T9_vSEKuKtM"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
